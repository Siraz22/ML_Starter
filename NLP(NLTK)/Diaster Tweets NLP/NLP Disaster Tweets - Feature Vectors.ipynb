{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,HashingVectorizer\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('train.csv')\n",
    "test_raw = pd.read_csv('test.csv')\n",
    "\n",
    "train_copy = train_raw.copy(deep = True)\n",
    "test_copy = test_raw.copy(deep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization Methods\n",
    "\n",
    "We take a dataset and convert it into a corpus. Then we create a vocabulary of all the unique words in the corpus. Using this vocabulary, we can then create a feature vector of the count of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 7,\n",
       " 'quick': 6,\n",
       " 'brown': 0,\n",
       " 'fox': 2,\n",
       " 'jumps': 3,\n",
       " 'over': 5,\n",
       " 'lazy': 4,\n",
       " 'dog': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['The quick brown fox', 'The quick brown fox jumps over a lazy dog']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences)\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 6457), (3263, 21637))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(train_copy['text'])\n",
    "test_vectors = count_vectorizer.transform(test_copy['text'])\n",
    "\n",
    "train_vectros.shape, test_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 21498), (3263, 21498))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "train_vectros = count_vectorizer.fit_transform(train_copy['text'])\n",
    "test_vectors = count_vectorizer.transform(test_copy['text'])\n",
    "\n",
    "train_vectros.shape, test_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min_DF and Max_DF parameter\n",
    "\n",
    "MIN_DF lets you ignore those terms that appear rarely in a corpus. In other words, if MIN_dfis 2, it means that a word has to occur at least two documents to be considered useful.\n",
    "\n",
    "MAX_DF on the other hand, ignores terms that have a document frequency strictly higher than the given threshold.These will be words which appear a lot of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 6457), (3263, 6457))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=stop_words, min_df=2, max_df=0.8)\n",
    "\n",
    "train_vectros = count_vectorizer.fit_transform(train_copy['text'])\n",
    "test_vectors = count_vectorizer.transform(test_copy['text'])\n",
    "\n",
    "train_vectros.shape, test_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text - REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 6457), (3263, 16569))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a custom preprocessor that lowercases, removes special characters, removes hyperlinks and punctuation\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) # remove special chars\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "count_vectorizer = CountVectorizer(list(train_copy['text']),preprocessor=custom_preprocessor)\n",
    "\n",
    "train_vectors = count_vectorizer.fit_transform(train_copy['text'])\n",
    "test_vectors = count_vectorizer.transform(test_copy['text'])\n",
    "\n",
    "train_vectros.shape, test_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 6457), (3263, 77963))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(list(train_copy['text']),preprocessor=custom_preprocessor, ngram_range=(1,2))\n",
    "\n",
    "train_vectors = count_vectorizer.fit_transform(train_copy['text'])\n",
    "test_vectors = count_vectorizer.transform(test_copy['text'])\n",
    "\n",
    "train_vectros.shape, test_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our',\n",
       " 'deeds',\n",
       " 'are',\n",
       " 'the',\n",
       " 'reason',\n",
       " 'of',\n",
       " 'this',\n",
       " 'earthquake',\n",
       " 'may',\n",
       " 'allah']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(count_vectorizer.vocabulary_)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 6457), (3263, 61394))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strict bigrams\n",
    "\n",
    "count_vectorizer = CountVectorizer(list(train_copy['text']),preprocessor=custom_preprocessor,ngram_range=(2,2))\n",
    "\n",
    "train_vectors = count_vectorizer.fit_transform(train_copy['text'])\n",
    "test_vectors = count_vectorizer.transform(test_copy['text'])\n",
    "\n",
    "train_vectros.shape, test_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our deeds',\n",
       " 'deeds are',\n",
       " 'are the',\n",
       " 'the reason',\n",
       " 'reason of',\n",
       " 'of this',\n",
       " 'this earthquake',\n",
       " 'earthquake may',\n",
       " 'may allah',\n",
       " 'allah forgive']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(count_vectorizer.vocabulary_)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 6457), (3263, 838))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# character level bigrams\n",
    "\n",
    "count_vectorizer = CountVectorizer(list(train_copy['text']),\n",
    "                                   preprocessor=custom_preprocessor,\n",
    "                                   ngram_range=(2,2), \n",
    "                                   analyzer = 'char_wb')\n",
    "\n",
    "train_vectors = count_vectorizer.fit_transform(train_copy['text'])\n",
    "test_vectors = count_vectorizer.transform(test_copy['text'])\n",
    "\n",
    "train_vectros.shape, test_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' o', 'ou', 'ur', 'r ', ' d', 'de', 'ee', 'ed', 'ds', 's ']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(count_vectorizer.vocabulary_)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Baseline Model with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=stop_words, preprocessor = custom_preprocessor,\n",
    "                                  ngram_range=(1,2))\n",
    "\n",
    "train_vectors = count_vectorizer.fit_transform(train_copy['text'])\n",
    "test_vectors = count_vectorizer.transform(test_copy['text'])\n",
    "\n",
    "X_train = train_vectors\n",
    "y_train = train_copy['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64890282 0.60940695 0.68431772 0.65236686 0.71437782]\n",
      "0.6618744355777822\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_scores = model_selection.cross_val_score(nb_classifier, \n",
    "                                         X_train,\n",
    "                                         y_train,\n",
    "                                         cv = 5, \n",
    "                                         scoring = 'f1')\n",
    "print(nb_scores)\n",
    "print(nb_scores.sum()/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59632139 0.5311943  0.62033898 0.52423343 0.70342523]\n",
      "0.595102666987082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=1.0)\n",
    "lr_scores = model_selection.cross_val_score(clf, X_train, y_train, cv=5, scoring=\"f1\")\n",
    "\n",
    "print(lr_scores)\n",
    "print(lr_scores.sum()/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_submission[\"target\"] = nb_classifier.predict(test_vectors)\n",
    "sample_submission.to_csv(\"FeatureVector_UnscaledNB.csv\", index=False)\n",
    "\n",
    "#RESULT - 0.79141 Accuracy instead of 0.78629 from the previous model. Definitely above the 63% bracket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Reg Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_submission[\"target\"] = clf.predict(test_vectors)\n",
    "sample_submission.to_csv(\"FeatureVector_UnscaledLR.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level\n",
    "tfidf = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}',max_features=5000)\n",
    "\n",
    "train_tfidf = tfidf.fit_transform(train_copy['text'])\n",
    "test_tfidf = tfidf.transform(test_copy[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram level\n",
    "tfidf = TfidfVectorizer(analyzer='word',ngram_range=(2,3),token_pattern=r'\\w{1,}',max_features=5000)\n",
    "\n",
    "train_tfidf = tfidf.fit_transform(train_copy['text'])\n",
    "test_tfidf = tfidf.transform(test_copy[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters level\n",
    "tfidf = TfidfVectorizer(analyzer='char',ngram_range=(2,3),token_pattern=r'\\w{1,}',max_features=5000)\n",
    "\n",
    "train_tfidf = tfidf.fit_transform(train_copy['text'])\n",
    "test_tfidf = tfidf.transform(test_copy[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf = tfidf.fit_transform(train_copy['text'])\n",
    "test_tfidf = tfidf.transform(test_copy[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64338235 0.61563255 0.63263598 0.61979167 0.74009509]\n",
      "0.6503075288852589\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1.0)\n",
    "lr_scores = model_selection.cross_val_score(clf, train_tfidf, train_copy[\"target\"], cv=5, scoring=\"f1\")\n",
    "\n",
    "print(lr_scores)\n",
    "print(lr_scores.sum()/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_tfidf, train_copy[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_submission[\"target\"] = clf.predict(test_tfidf)\n",
    "sample_submission.to_csv(\"FeatureVector_ScaledLR.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('gpu': conda)",
   "language": "python",
   "name": "python37764bitgpuconda08766899e99640f8bf22a5cda132f1fd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
